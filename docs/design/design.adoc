= Design Docment

This document is to describe the NtmOS Design

== Running HW Assumption

The NtmOS will assume the following hardware configuration:

. x86-32 CPU
. RAM >= 256MB
. Floppy Disk

== Memory Management

=== Global Page Table

Now in the implementation the paging is enabled, but due to making each process has its own page structure is a bit complex, so in current phase I decide that even though the paging is enabled, but I will make the processes to share the one global paging table. Base on this and to make the process protection, I will continue to use the segment to do the process memory isolation and protection.

In this global paging mode, we will do the following virtual address to physical address mapping:

[options="header", title="Memory Address Mapping"]
|=========
| Virtual Address Start Address | Virtual Address End Address | Physical Address Start Address | Physical Address End Address | Comment
| 0x00000000 | 0x07FFFFFF | 0x00000000 | 0x07FFFFFF | The first physical 128MB RAM is identity page mapping
| 0x80000000 | 0x87FFFFFF | 0x08000000 | 0x0FFFFFFF | The second physical 256MB RAM is mapped to kernel space 
|=========

=== Kernel Space Memory Layout 

This section documents the kernel virtual space memory layout, the below table documents the layout:

[options="header", title="Kernel Virtual Space Memory Layout"]
|==========
| Virtual Address Start Address | Virtual Address End Address | Size | Usage | Non-Swap | Comment
| 0x80000000 | 0x80100000 | 1MB | IDT + GDT | true |  
| 0x80100000 | 0x80200000 | 1MB | Kernel executable | true |
| 0x80200000 | 0x80300000 | 1MB | Init kernel stack | true | from 0x80300000 down to start address
| 0x80400000 | 0x80C00000 | 8MB | Global page table | true |
| 0x80C00000 | N/A | N/A | Kernel heap | true | 
|==========

I am still considering on how to fit the page frame bitmap into the kernel space table, suppose we need to support up to 4GB RAM, and for each page frame (4KB) we need 1 byte to store the frame allocation status, then it just need 1MB memory to for the whole page frame bitmap table.

NOTE: I may need to move the kernel heap to after the global page table so that it can have more space.

Some of the table size:

For the size of IDT table:  256 * 8 = 2048 = 2KB

For the size of the GDT table:  8192 * 8 = 65536 = 64KB

As we have 1MB for the memeory space of IDT+GDT, so in this region we can have sub memory layout as:

[options="header", title="IDT + GDT Memory Region Layout"]
|=========
| Virtual Address Start Address | Size | Usage | Comment
| 0x80000000 | 64KB | IDT | For IDT table, reserved 64KB, actually only the first 2KB are needed, with 256 IDT entries
| 0x80010000 | 64KB | GDT | For GDT table, with 8192 GDT entries
|=========

=== Memory Layout for applications ===

Currently the physical address from [0x00000000, 0x08000000) are not used by the kernel, so we can use this range for application memory.

But as we know that there are some of the memory hole before 64MB (also some address are mapped to special functions such as text mode display buffer), so for safety we will use the physical address from [0x04000000, 0x08000000) for application memory.

The idea is that, each application will be allocated by 1MB memory space, so we can have 64 applications to be running at the same time (as now we don't support sophisticated paging), we will enable the LDT so that each application will have its own LDT to restrict the memory access.

And in this 1MB memory, the first 512KB will be located to code + data, and the second 512KB will be located to userspace stack (grow in downwards direction).

So let's start the implementation in a native way:

We have a 64 elements array maintined by kernel, each corresponding a 1MB slot in [64MB, 128MB), the element identify whether the corresponding slot is used by any process or not, the task_t structure will be expand to include the index in this array, and the task_t also need to expand to include LDT table, we shall also set the LDT address in task's TSS table accordingly.

Once this get done, we copy the simpleapp's binary code to the corresponding memory, and then use the way descripted in https://wiki.osdev.org/Getting_to_Ring_3[] to make the kernel jump to the userspace.

Easy? Let's get it done.


=== GDT Layout ===

This section describe the GDT layout.

At current stage the GDT is planned to have the following layout:

[options="header", title="GDT Layout"]
|====
| GDT Entry | Usage
| 0 | No used
| 1 | System code segment
| 2 | System data sgement
| 3 - 15 | reserved
| 16 - 79 | TSS for processes
| 80 - 143 | LDT for processes
| 144 - 8191 | Unallocated
|==== 

=== LDT Layout ===

Each process has its own LDT for their userspace code and data, the the process LDT will have the following layout:

[options="header", title="LDT Layout"]
|====
| LDT Entry | Usage
| 0 | userspace code segment
| 1 | userspace data segment
|====

